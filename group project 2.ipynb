{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DS_group_project.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e08659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # Remove emojis (if applicable)\n",
    "    # Add the code to remove emojis here\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df[\"REVIEW\"] = df[\"REVIEW\"].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "\n",
    "\n",
    "df[\"REVIEW\"] = df[\"REVIEW\"].apply(lambda x: clean_text(x))\n",
    "df[\"REVIEW\"] = df[\"REVIEW\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "df[\"REVIEW\"] = df[\"REVIEW\"].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "def build_vocabulary(reviews):\n",
    "    vocabulary = defaultdict(int)\n",
    "    for review in reviews:\n",
    "        for word in review:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(df[\"REVIEW\"])\n",
    "\n",
    "def review_to_bow(review, vocabulary):\n",
    "    bow = defaultdict(int)\n",
    "    for word in review:\n",
    "        if word in vocabulary:\n",
    "            bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "df[\"REVIEW\"] = df[\"REVIEW\"].apply(lambda x: review_to_bow(x, vocabulary))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# replace NaN values with 0\n",
    "df.fillna(value=0, inplace=True)\n",
    "\n",
    "# replace infinite values with a large finite value\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(value=1e10, inplace=True)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# convert bag of words to a numerical representation using DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"REVIEW\"])\n",
    "y = df[\"RATING\"]\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# train the logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# train a naive bayes model on the training data\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train , y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "naivebayesaccuracy = naive_bayes.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy,\"\\nNaive Bayes Accuracy: \",naivebayesaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd658c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rosie's models: linear regression, single perceptron and multilayer perceptron net\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# making a binary sentiment column so that the Perceptron can process the input\n",
    "\n",
    "def sentiment_processor_binary(sentiment_list):\n",
    "    binary_sentiment = []\n",
    "    for item in sentiment_list:\n",
    "        if item >= 5:\n",
    "            binary_sentiment.append(1)\n",
    "        else:\n",
    "            binary_sentiment.append(0)\n",
    "    return binary_sentiment\n",
    "\n",
    "df[\"BINARY_SENTIMENT\"] = sentiment_processor_binary(df[\"RATING\"])\n",
    "\n",
    "# splitting the dataset\n",
    "\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(df[\"REVIEW\"], df[\"BINARY_SENTIMENT\"], \n",
    "                                                                              test_size = 0.1, random_state=156)\n",
    "\n",
    "# turning the reviews into a tf-idf array\n",
    "\n",
    "# vectorise and remove stopwords\n",
    "\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", lowercase=True, norm=\"l1\")\n",
    "\n",
    "# run on training and testing reviews\n",
    "\n",
    "train_features = vectoriser.fit_transform(train_features)\n",
    "test_features = vectoriser.transform(test_features)\n",
    "\n",
    "# building a basic Perceptron and testing it, printing the accuracy.\n",
    "\n",
    "reviews_perceptron = Perceptron()\n",
    "reviews_perceptron.fit(train_features, train_targets)\n",
    "accuracy = reviews_perceptron.score(test_features, test_targets)\n",
    "\n",
    "print(f\"The accuracy for a regular Perceptron is {accuracy}.\")\n",
    "\n",
    "# building a multilayer neural net and testing it, printing accuracy.\n",
    "\n",
    "# we will need to create several different models so I am creating a function for max efficiency.\n",
    "\n",
    "print(\"The next results are relevant to the multilayer Perceptron neural net.\")\n",
    "\n",
    "def create_neural_net(train_features, train_targets, test_features, test_targets, talking = False, \n",
    "                      neurons = 2, iterations = 200):\n",
    "    neural_net = MLPClassifier(hidden_layer_sizes = neurons, max_iter = iterations, activation = \"relu\", \n",
    "                               solver = \"sgd\", random_state = 800, learning_rate = \"adaptive\", verbose = talking)\n",
    "    neural_net.fit(train_features, train_targets)\n",
    "    neural_net_predictions = neural_net.predict(test_features)\n",
    "    neural_net_accuracy = metrics.accuracy_score(test_targets, neural_net_predictions)\n",
    "\n",
    "    print(f\"The accuracy for {neurons} neurons per hidden layer and {iterations} max iterations is\" +\n",
    "          f\" {neural_net_accuracy}.\")\n",
    "    \n",
    "# testing out another neural net as the \"adam\" solver is good for large datasets with thousands of entries.\n",
    "\n",
    "def create_adam(train_features, train_targets, test_features, test_targets, talking = False, \n",
    "                neurons = 2, iterations = 200):\n",
    "    neural_net = MLPClassifier(hidden_layer_sizes = neurons, max_iter = iterations, activation = \"relu\", \n",
    "                               solver = \"adam\", random_state = 800, verbose = talking)\n",
    "    neural_net.fit(train_features, train_targets)\n",
    "    neural_net_predictions = neural_net.predict(test_features)\n",
    "    neural_net_accuracy = metrics.accuracy_score(test_targets, neural_net_predictions)\n",
    "    \n",
    "    print(f\"The accuracy for {neurons} neurons per hidden layer and {iterations} max iterations is\" +\n",
    "          f\" {neural_net_accuracy}.\")\n",
    "    \n",
    "# creating a simple neural net with default values\n",
    "\n",
    "create_neural_net(train_features, train_targets, test_features, test_targets)\n",
    "\n",
    "# creating a more complex neural net\n",
    "\n",
    "create_adam(train_features, train_targets, test_features, test_targets, True, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97a50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dc19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
